name: Generate M3U from ChuoiChien
on:
  workflow_dispatch:
  schedule:
    - cron: '*/20 * * * *'

jobs:
  build:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - uses: actions/checkout@v3

      - name: Install Dependencies
        run: pip install curl_cffi beautifulsoup4

      - name: Run ChuoiChien Scraper
        run: |
          cat << 'EOF' > scraper.py
          from curl_cffi import requests
          from bs4 import BeautifulSoup
          import re
          import unicodedata

          TARGET_URL = "https://live16.chuoichientv.com/lich-thi-dau"
          URL_PREFIX = "https://gckc0525.edgemaxcdn.org/live/"
          URL_SUFFIX = ".flv"

          def generate_stream_id(blv_name):
              name = blv_name.strip().lower()
              no_accent = unicodedata.normalize('NFKD', name).encode('ASCII', 'ignore').decode('utf-8')
              return re.sub(r'[^a-z0-9]', '', no_accent)

          try:
              print(f"ƒêang c√†o d·ªØ li·ªáu t·ª´: {TARGET_URL}")
              response = requests.get(TARGET_URL, impersonate="chrome110", timeout=30)
              html = response.text
              
              if len(html) < 2000 or "Just a moment" in html or "Cloudflare" in html:
                  print("‚ö†Ô∏è B·ªã Cloudflare ch·∫∑n!")
              
              soup = BeautifulSoup(html, 'html.parser')
              m3u_lines = ["#EXTM3U"]
              count = 0
              
              # T√åM T·∫§T C·∫¢ LINK C√ì CH·ª®A CH·ªÆ /live/ V√Ä M√É S·ªê TR·∫¨N
              matches = soup.find_all('a', href=re.compile(r'/live/\d+'))
              print(f"üîç T√¨m th·∫•y {len(matches)} kh·ªëi tr·∫≠n ƒë·∫•u. B·∫Øt ƒë·∫ßu b√≥c t√°ch...")
              
              for match in matches:
                  match_link = match.get('href', 'Kh√¥ng r√µ link')
                  try:
                      # 1. B√ìC T√äN ƒê·ªòI QUA TH·∫∫ ·∫¢NH LOGO (Tuy·ªát ƒë·ªëi kh√¥ng tr∆∞·ª£t)
                      imgs = match.find_all('img')
                      # L·ªçc l·∫•y ·∫£nh c√≥ t√™n, v√† lo·∫°i tr·ª´ ·∫£nh c·ªßa √¥ng BLV
                      team_names = [img.get('alt', '').strip() for img in imgs if img.get('alt') and img.get('alt') != 'BLV Avatar']
                      
                      if len(team_names) < 2:
                          print(f"‚è≠Ô∏è B·ªè qua (Thi·∫øu logo 2 ƒë·ªôi): {match_link}")
                          continue
                          
                      team1, team2 = team_names[0], team_names[1]
                      match_name = f"{team1} vs {team2}"

                      # 2. B√ìC T√äN BLV
                      blv_span = match.find('span', class_=re.compile(r'blv-info'))
                      if not blv_span:
                          print(f"‚è≠Ô∏è B·ªè qua (Ch∆∞a ph√¢n c√¥ng BLV): {match_name}")
                          continue
                          
                      blv_name = blv_span.get_text(strip=True)
                      if not blv_name:
                          print(f"‚è≠Ô∏è B·ªè qua (T√™n BLV r·ªóng): {match_name}")
                          continue

                      # 3. B√ìC GI·ªú ƒê√Å B·∫∞NG REGEX XUY√äN TH·ª¶NG VƒÇN B·∫¢N
                      match_text = match.get_text(separator=' ')
                      time_match = re.search(r'(\d{2}:\d{2})\s+(\d{2}/\d{2})', match_text)
                      if time_match:
                          time_str = f"{time_match.group(1).replace(':', 'h')}-{time_match.group(2)}"
                      else:
                          time_str = "00:00"

                      # 4. √âP LINK V√Ä L∆ØU
                      stream_id = generate_stream_id(blv_name)
                      stream_url = f"{URL_PREFIX}{stream_id}{URL_SUFFIX}"

                      display_text = f"ChuoiChien {time_str}: [{blv_name}] {match_name}"
                      m3u_lines.append(f'#EXTINF:-1 group-title="ChuoiChien",{display_text}')
                      m3u_lines.append(stream_url)
                      count += 1
                      print(f"‚úÖ H·ªët th√†nh c√¥ng: {match_name} -> ID: {stream_id}")
                      
                  except Exception as ex:
                      print(f"‚ùå L·ªói k·∫πt m·∫°ng t·∫°i tr·∫≠n {match_link}: {ex}")
                      continue

              with open('chuoichientv.m3u', 'w', encoding='utf-8') as f:
                  f.write("\n".join(m3u_lines))
                  
              print(f"üéâ T·ªîNG K·∫æT: ƒê√£ ƒë√≥ng g√≥i th√†nh c√¥ng {count} lu·ªìng ph√°t!")

          except Exception as e:
              print(f"‚ùå L·ªói h·ªá th·ªëng: {e}")
              with open('chuoichientv.m3u', 'w', encoding='utf-8') as f:
                  f.write("#EXTM3U\n")
          EOF
          
          python3 scraper.py

      - name: Commit and Push
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git add chuoichientv.m3u
          git commit -m "Auto Scrape ChuoiChien TV Bulletproof" || echo "No changes"
          git push
