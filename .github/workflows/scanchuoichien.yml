name: Generate M3U from ChuoiChien
on:
  workflow_dispatch:
  schedule:
    - cron: '*/20 * * * *'

jobs:
  build:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - uses: actions/checkout@v3

      - name: Install Dependencies
        run: pip install curl_cffi

      - name: Run ChuoiChien API Scraper
        run: |
          cat << 'EOF' > scraper.py
          from curl_cffi import requests
          import json
          from datetime import datetime
          import unicodedata
          import re

          API_URL = "https://api.chuoichientv.com/v1/matches?page=1&limit=100&sport=&type=blv"

          try:
              print(f"ƒêang ƒë√¢m th·∫≥ng v√†o API: {API_URL}")
              
              headers = {
                  "Authorization": "Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJndWVzdElkIjoiZTM0Zjk3ZmQtNWMxMC00MGEzLWE1OGYtZDE3MmQwMmIxNDZjIiwidHlwZSI6Imd1ZXN0IiwiaXAiOiIxNjIuMTU5Ljk4LjIyMCIsInVzZXJBZ2VudCI6Ik1vemlsbGEvNS4wIChXaW5kb3dzIE5UIDEwLjA7IFdpbjY0OyB4NjQpIEFwcGxlV2ViS2l0LzUzNy4zNiAoS0hUTUwsIGxpa2UgR2Vja28pIENocm9tZS8xMzEuMC4wLjAgU2FmYXJpLzUzNy4zNiIsIm5hbWUiOiJCw7puIMSQ4buPIDQ1MyIsInRpbWVzdGFtcCI6MTc3MjI5MTc4NzEwNCwiaWF0IjoxNzcyMjkxNzg3LCJleHAiOjE4MDM4Mjc3ODd9.iHhwdQaDRcrjyRfCVGCbSZb6dFj-EuzJblTD1wmttV0",
                  "Origin": "https://live16.chuoichientv.com",
                  "Referer": "https://live16.chuoichientv.com/",
                  "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"
              }

              response = requests.get(API_URL, headers=headers, impersonate="chrome110", timeout=30)
              data = response.json()
              
              # L·∫§Y ƒê√öNG T√äN BI·∫æN T·ª™ LOG C·ª¶A B√ÅC
              matches = data.get('matches', [])

              m3u_lines = ["#EXTM3U"]
              count = 0
              
              for match in matches:
                  try:
                      # 1. L·∫•y t√™n 2 ƒë·ªôi (x·ª≠ l√Ω m·ªçi tr∆∞·ªùng h·ª£p web ƒë·ªïi t√™n bi·∫øn)
                      home = match.get('home', {}).get('name') or match.get('homeTeam', {}).get('name') or "Home"
                      away = match.get('away', {}).get('name') or match.get('awayTeam', {}).get('name') or "Away"
                      match_name = match.get('name') or f"{home} vs {away}"

                      # 2. L·∫•y gi·ªù ƒë√°
                      time_str = "00h00"
                      timestamp = match.get('timestamp') or match.get('matchTime') or match.get('time')
                      if timestamp:
                          if isinstance(timestamp, (int, float)):
                              if timestamp > 1e11: timestamp /= 1000
                              dt = datetime.fromtimestamp(timestamp)
                              time_str = dt.strftime("%Hh%M-%d/%m")
                          elif isinstance(timestamp, str):
                              time_str = timestamp[:16].replace('T', ' ').replace(':', 'h')

                      # 3. L·∫•y th√¥ng tin BLV v√† Link M3U8
                      blvs = match.get('blvs', [])
                      if not blvs:
                          continue # Tr·∫≠n n√†o ch∆∞a g√°n BLV th√¨ b·ªè qua
                          
                      blv = blvs[0] # L·∫•y √¥ng BLV ƒë·∫ßu ti√™n
                      blv_name = blv.get('name', 'BLV Chu·ªëi')

                      # K·ª≤ T√çCH: L·∫•y tr·ª±c ti·∫øp link M3U8 HD1 si√™u x·ªãn t·ª´ API
                      streams = blv.get('streams', [])
                      if streams:
                          # L·∫•y url c·ªßa lu·ªìng ƒë·∫ßu ti√™n (th∆∞·ªùng l√† HD1)
                          stream_url = streams[0].get('url')
                      else:
                          # Ph∆∞∆°ng √°n d·ª± ph√≤ng n·∫øu API ·∫©n link stream (t·ª± ch·∫ø link c≈©)
                          username = str(blv.get('username', '')).strip().lower()
                          if not username:
                              no_accent = unicodedata.normalize('NFKD', blv_name).encode('ASCII', 'ignore').decode('utf-8')
                              username = re.sub(r'[^a-z0-9]', '', no_accent.lower())
                          stream_url = f"https://gckc0525.edgemaxcdn.org/live/{username}.flv"

                      if not stream_url:
                          continue

                      # 4. Xu·∫•t file
                      display_text = f"ChuoiChien {time_str}: [{blv_name}] {match_name}"
                      m3u_lines.append(f'#EXTINF:-1 group-title="ChuoiChien",{display_text}')
                      m3u_lines.append(stream_url)
                      count += 1
                      
                  except Exception as ex:
                      print(f"L·ªói ph√¢n t√≠ch 1 tr·∫≠n: {ex}")
                      continue

              with open('chuoichientv.m3u', 'w', encoding='utf-8') as f:
                  f.write("\n".join(m3u_lines))
                  
              print(f"üéâ T·ªîNG K·∫æT: R√öT TH√ÄNH C√îNG T·∫¨N G·ªêC {count} LU·ªíNG PH√ÅT CHU·ªêI CHI√äN TV!")

          except Exception as e:
              print(f"‚ùå L·ªói h·ªá th·ªëng: {e}")
              with open('chuoichientv.m3u', 'w', encoding='utf-8') as f:
                  f.write("#EXTM3U\n")
          EOF
          
          python3 scraper.py

      - name: Commit and Push
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git add chuoichientv.m3u
          git commit -m "Auto Scrape ChuoiChien TV via API JSON - Full Unlocked" || echo "No changes"
          git push
